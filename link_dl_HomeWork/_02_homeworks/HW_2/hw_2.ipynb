{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.013451Z",
     "start_time": "2024-10-25T09:15:46.000890Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "\n",
    "# 현재 작업 디렉토리로 설정 (Jupyter Notebook 호환)\n",
    "CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "# TitanicDataset: 학습 및 검증 데이터를 위한 Dataset 클래스 정의\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 입력 데이터를 float 타입의 텐서로 변환하여 저장\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        # 타겟 레이블 데이터를 long 타입의 텐서로 변환하여 저장 (분류 문제를 위한 정수형)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 샘플 개수 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 샘플의 입력 데이터와 타겟 레이블 반환\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        # 딕셔너리 형태로 반환하여 DataLoader에서 사용 가능\n",
    "        return {'input': feature, 'target': target}\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보 문자열로 출력 (데이터 크기, 입력, 타겟 형상 포함)\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n",
    "        return str"
   ],
   "outputs": [],
   "execution_count": 419
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.089284Z",
     "start_time": "2024-10-25T09:15:46.077285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TitanicTestDataset: 테스트 데이터를 위한 Dataset 클래스 정의 (타겟이 없기 때문에 input만 필요)\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # 입력 데이터를 float 타입의 텐서로 변환하여 저장\n",
    "        self.X = torch.FloatTensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 샘플 개수 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 샘플의 입력 데이터 반환\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "\n",
    "    def __str__(self):\n",
    "        # 테스트 데이터셋 정보 문자열로 출력 (데이터 크기와 입력 형상 포함)\n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )\n",
    "        return str"
   ],
   "id": "f0fe78596f1f59d0",
   "outputs": [],
   "execution_count": 420
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.149536Z",
     "start_time": "2024-10-25T09:15:46.137525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋을 전처리하여 학습, 검증, 테스트 데이터셋으로 분할하는 함수\n",
    "def get_preprocessed_dataset():\n",
    "    # 학습 및 테스트 데이터 파일 경로 지정\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    # 데이터 파일 읽기\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    # 학습 및 테스트 데이터 결합 (전처리를 위해)\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 여러 단계의 전처리 적용\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # 학습 데이터와 타겟 분리\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    # TitanicDataset 객체 생성하여 학습, 검증, 테스트 데이터셋으로 분할\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ],
   "id": "35c3d023e47c6c2e",
   "outputs": [],
   "execution_count": 421
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.225709Z",
     "start_time": "2024-10-25T09:15:46.212702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pclass별로 평균 Fare 값을 계산하여 결측치를 채우는 전처리 함수\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass(등급)별 평균 Fare 계산\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    # Fare의 결측치를 등급별 평균 Fare 값으로 채움\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df"
   ],
   "id": "5ce6266aac0bdd7f",
   "outputs": [],
   "execution_count": 422
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.256365Z",
     "start_time": "2024-10-25T09:15:46.237728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 이름(Name)을 사용하여 성(family_name), 호칭(honorific), 이름(name)을 분리하는 전처리 함수\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # 이름을 쉼표(,)와 점(.)을 기준으로 분리하여 세 개의 컬럼 생성\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    # 각 문자열을 공백 제거하여 정리\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    # 원래 데이터프레임에 분리된 이름 데이터를 추가\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df"
   ],
   "id": "79f18a36740ee5e7",
   "outputs": [],
   "execution_count": 423
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.316752Z",
     "start_time": "2024-10-25T09:15:46.291202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# honorific(호칭)별로 나이의 중앙값을 계산하여 Age 결측치를 채우는 전처리 함수\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\"]\n",
    "    # 호칭별 중앙값으로 Age의 결측치를 채움\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    # 필요 없는 열 제거\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df"
   ],
   "id": "4db2d99401b3142a",
   "outputs": [],
   "execution_count": 424
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.361762Z",
     "start_time": "2024-10-25T09:15:46.350767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 가족 수(family_num)와 혼자 탑승 여부(alone)를 나타내는 컬럼 추가 및 불필요한 열 제거\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족 수(family_num) 계산하여 새로운 컬럼에 저장\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    # 가족 수가 0이면 혼자 탑승한 것으로 간주하여 alone 컬럼에 1을 할당\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    # 가족이 있으면 alone 값을 0으로 설정\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    # 필요 없는 열(식별자, 이름 등) 삭제\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df"
   ],
   "id": "3cf49404a01229af",
   "outputs": [],
   "execution_count": 425
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.437729Z",
     "start_time": "2024-10-25T09:15:46.420729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# honorific 값의 개수를 줄여, 자주 등장하지 않는 값들은 \"other\"로 묶음\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # 주요 호칭 이외의 값들을 \"other\"로 변환하여 하나로 묶음\n",
    "    all_df.loc[\n",
    "        ~((all_df[\"honorific\"] == \"Mr\") |\n",
    "          (all_df[\"honorific\"] == \"Miss\") |\n",
    "          (all_df[\"honorific\"] == \"Mrs\") |\n",
    "          (all_df[\"honorific\"] == \"Master\")),\n",
    "        \"honorific\"] = \"other\"\n",
    "    # Embarked(탑승 항구)의 결측치를 \"missing\"으로 채움\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df"
   ],
   "id": "70e8044c25566fc2",
   "outputs": [],
   "execution_count": 426
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.497733Z",
     "start_time": "2024-10-25T09:15:46.486752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 범주형 변수들을 LabelEncoder를 사용해 수치형으로 변환하는 함수\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 데이터프레임에서 문자열 타입인 컬럼들만 추출\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        # 각 범주형 변수를 LabelEncoder로 수치형으로 변환\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "            le = le.fit(all_df[category_feature])\n",
    "            all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df"
   ],
   "id": "8f7068b8f97ebf91",
   "outputs": [],
   "execution_count": 427
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.573298Z",
     "start_time": "2024-10-25T09:15:46.561279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "# 신경망 모델 정의\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        # 신경망 구조 정의 (은닉층 2개, 각 층 30개의 유닛)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),  # 입력층 -> 첫 번째 은닉층\n",
    "            nn.ReLU(),               # 활성화 함수로 ReLU 사용\n",
    "            nn.Linear(30, 30),       # 첫 번째 은닉층 -> 두 번째 은닉층\n",
    "            nn.ReLU(),               # ReLU 활성화 함수\n",
    "            nn.Linear(30, n_output), # 두 번째 은닉층 -> 출력층\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 x를 모델에 통과시켜 최종 출력값 반환\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "id": "65e9fe6b1aa76643",
   "outputs": [],
   "execution_count": 428
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:46.603348Z",
     "start_time": "2024-10-25T09:15:46.584279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 테스트 함수: 테스트 데이터셋을 통해 모델의 예측을 출력\n",
    "def test(test_data_loader):\n",
    "    print(\"[TEST]\")\n",
    "    # 테스트 데이터에서 첫 번째 배치를 가져옴\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    # 테스트를 위한 모델 생성 (입력 노드 11개, 출력 노드 2개)\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "    # 모델을 통해 배치의 입력 데이터를 예측\n",
    "    output_batch = my_model(batch['input'])\n",
    "    # 출력의 각 샘플에서 가장 높은 확률의 클래스를 예측값으로 선택\n",
    "    prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "    # 예측 결과 출력 (테스트 데이터의 인덱스와 예측값)\n",
    "    for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "        print(idx, prediction.item())"
   ],
   "id": "ec5f8e04c4785372",
   "outputs": [],
   "execution_count": 429
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:15:48.536138Z",
     "start_time": "2024-10-25T09:15:46.651028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 메인 함수\n",
    "if __name__ == \"__main__\":\n",
    "    # 전처리된 데이터셋을 가져와 학습, 검증, 테스트 데이터셋으로 분리\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "    # 데이터셋 크기 출력\n",
    "    print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "        len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "    ))\n",
    "\n",
    "    # DataLoader를 사용해 학습, 검증, 테스트 데이터 로드\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 학습 데이터 로드 확인\n",
    "    print(\"[TRAIN]\")\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "    # 검증 데이터 로드 확인\n",
    "    print(\"[VALIDATION]\")\n",
    "    for idx, batch in enumerate(validation_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "    # 테스트 함수 호출하여 테스트 데이터의 예측 결과 확인\n",
    "    test(test_data_loader)"
   ],
   "id": "9248d4c6440d1f36",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alstj\\AppData\\Local\\Temp\\ipykernel_16408\\928423758.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\alstj\\AppData\\Local\\Temp\\ipykernel_16408\\377400784.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 713, validation_dataset.shape: 178, test_dataset: 418\n",
      "[TRAIN]\n",
      "0 - torch.Size([16, 11]): torch.Size([16])\n",
      "1 - torch.Size([16, 11]): torch.Size([16])\n",
      "2 - torch.Size([16, 11]): torch.Size([16])\n",
      "3 - torch.Size([16, 11]): torch.Size([16])\n",
      "4 - torch.Size([16, 11]): torch.Size([16])\n",
      "5 - torch.Size([16, 11]): torch.Size([16])\n",
      "6 - torch.Size([16, 11]): torch.Size([16])\n",
      "7 - torch.Size([16, 11]): torch.Size([16])\n",
      "8 - torch.Size([16, 11]): torch.Size([16])\n",
      "9 - torch.Size([16, 11]): torch.Size([16])\n",
      "10 - torch.Size([16, 11]): torch.Size([16])\n",
      "11 - torch.Size([16, 11]): torch.Size([16])\n",
      "12 - torch.Size([16, 11]): torch.Size([16])\n",
      "13 - torch.Size([16, 11]): torch.Size([16])\n",
      "14 - torch.Size([16, 11]): torch.Size([16])\n",
      "15 - torch.Size([16, 11]): torch.Size([16])\n",
      "16 - torch.Size([16, 11]): torch.Size([16])\n",
      "17 - torch.Size([16, 11]): torch.Size([16])\n",
      "18 - torch.Size([16, 11]): torch.Size([16])\n",
      "19 - torch.Size([16, 11]): torch.Size([16])\n",
      "20 - torch.Size([16, 11]): torch.Size([16])\n",
      "21 - torch.Size([16, 11]): torch.Size([16])\n",
      "22 - torch.Size([16, 11]): torch.Size([16])\n",
      "23 - torch.Size([16, 11]): torch.Size([16])\n",
      "24 - torch.Size([16, 11]): torch.Size([16])\n",
      "25 - torch.Size([16, 11]): torch.Size([16])\n",
      "26 - torch.Size([16, 11]): torch.Size([16])\n",
      "27 - torch.Size([16, 11]): torch.Size([16])\n",
      "28 - torch.Size([16, 11]): torch.Size([16])\n",
      "29 - torch.Size([16, 11]): torch.Size([16])\n",
      "30 - torch.Size([16, 11]): torch.Size([16])\n",
      "31 - torch.Size([16, 11]): torch.Size([16])\n",
      "32 - torch.Size([16, 11]): torch.Size([16])\n",
      "33 - torch.Size([16, 11]): torch.Size([16])\n",
      "34 - torch.Size([16, 11]): torch.Size([16])\n",
      "35 - torch.Size([16, 11]): torch.Size([16])\n",
      "36 - torch.Size([16, 11]): torch.Size([16])\n",
      "37 - torch.Size([16, 11]): torch.Size([16])\n",
      "38 - torch.Size([16, 11]): torch.Size([16])\n",
      "39 - torch.Size([16, 11]): torch.Size([16])\n",
      "40 - torch.Size([16, 11]): torch.Size([16])\n",
      "41 - torch.Size([16, 11]): torch.Size([16])\n",
      "42 - torch.Size([16, 11]): torch.Size([16])\n",
      "43 - torch.Size([16, 11]): torch.Size([16])\n",
      "44 - torch.Size([9, 11]): torch.Size([9])\n",
      "[VALIDATION]\n",
      "0 - torch.Size([16, 11]): torch.Size([16])\n",
      "1 - torch.Size([16, 11]): torch.Size([16])\n",
      "2 - torch.Size([16, 11]): torch.Size([16])\n",
      "3 - torch.Size([16, 11]): torch.Size([16])\n",
      "4 - torch.Size([16, 11]): torch.Size([16])\n",
      "5 - torch.Size([16, 11]): torch.Size([16])\n",
      "6 - torch.Size([16, 11]): torch.Size([16])\n",
      "7 - torch.Size([16, 11]): torch.Size([16])\n",
      "8 - torch.Size([16, 11]): torch.Size([16])\n",
      "9 - torch.Size([16, 11]): torch.Size([16])\n",
      "10 - torch.Size([16, 11]): torch.Size([16])\n",
      "11 - torch.Size([2, 11]): torch.Size([2])\n",
      "[TEST]\n",
      "torch.Size([418, 11])\n",
      "892 1\n",
      "893 1\n",
      "894 1\n",
      "895 1\n",
      "896 1\n",
      "897 1\n",
      "898 1\n",
      "899 1\n",
      "900 1\n",
      "901 1\n",
      "902 1\n",
      "903 1\n",
      "904 1\n",
      "905 1\n",
      "906 1\n",
      "907 1\n",
      "908 1\n",
      "909 1\n",
      "910 1\n",
      "911 1\n",
      "912 1\n",
      "913 1\n",
      "914 1\n",
      "915 1\n",
      "916 1\n",
      "917 1\n",
      "918 1\n",
      "919 1\n",
      "920 1\n",
      "921 1\n",
      "922 1\n",
      "923 1\n",
      "924 1\n",
      "925 1\n",
      "926 1\n",
      "927 1\n",
      "928 1\n",
      "929 1\n",
      "930 1\n",
      "931 1\n",
      "932 1\n",
      "933 1\n",
      "934 1\n",
      "935 1\n",
      "936 1\n",
      "937 1\n",
      "938 1\n",
      "939 1\n",
      "940 1\n",
      "941 1\n",
      "942 1\n",
      "943 1\n",
      "944 1\n",
      "945 1\n",
      "946 1\n",
      "947 1\n",
      "948 1\n",
      "949 1\n",
      "950 1\n",
      "951 1\n",
      "952 1\n",
      "953 1\n",
      "954 1\n",
      "955 1\n",
      "956 1\n",
      "957 1\n",
      "958 1\n",
      "959 1\n",
      "960 1\n",
      "961 1\n",
      "962 1\n",
      "963 1\n",
      "964 1\n",
      "965 1\n",
      "966 1\n",
      "967 1\n",
      "968 1\n",
      "969 1\n",
      "970 1\n",
      "971 1\n",
      "972 1\n",
      "973 1\n",
      "974 1\n",
      "975 1\n",
      "976 1\n",
      "977 1\n",
      "978 1\n",
      "979 1\n",
      "980 1\n",
      "981 1\n",
      "982 1\n",
      "983 1\n",
      "984 1\n",
      "985 1\n",
      "986 1\n",
      "987 1\n",
      "988 1\n",
      "989 1\n",
      "990 1\n",
      "991 1\n",
      "992 1\n",
      "993 1\n",
      "994 1\n",
      "995 1\n",
      "996 1\n",
      "997 1\n",
      "998 1\n",
      "999 1\n",
      "1000 1\n",
      "1001 1\n",
      "1002 1\n",
      "1003 1\n",
      "1004 1\n",
      "1005 1\n",
      "1006 1\n",
      "1007 1\n",
      "1008 1\n",
      "1009 1\n",
      "1010 1\n",
      "1011 1\n",
      "1012 1\n",
      "1013 1\n",
      "1014 1\n",
      "1015 1\n",
      "1016 1\n",
      "1017 1\n",
      "1018 1\n",
      "1019 1\n",
      "1020 1\n",
      "1021 1\n",
      "1022 1\n",
      "1023 1\n",
      "1024 1\n",
      "1025 1\n",
      "1026 1\n",
      "1027 1\n",
      "1028 1\n",
      "1029 1\n",
      "1030 1\n",
      "1031 1\n",
      "1032 1\n",
      "1033 1\n",
      "1034 1\n",
      "1035 1\n",
      "1036 1\n",
      "1037 1\n",
      "1038 1\n",
      "1039 1\n",
      "1040 1\n",
      "1041 1\n",
      "1042 1\n",
      "1043 1\n",
      "1044 1\n",
      "1045 1\n",
      "1046 1\n",
      "1047 1\n",
      "1048 1\n",
      "1049 1\n",
      "1050 1\n",
      "1051 1\n",
      "1052 1\n",
      "1053 1\n",
      "1054 1\n",
      "1055 1\n",
      "1056 1\n",
      "1057 1\n",
      "1058 1\n",
      "1059 1\n",
      "1060 1\n",
      "1061 1\n",
      "1062 1\n",
      "1063 1\n",
      "1064 1\n",
      "1065 1\n",
      "1066 1\n",
      "1067 1\n",
      "1068 1\n",
      "1069 1\n",
      "1070 1\n",
      "1071 1\n",
      "1072 1\n",
      "1073 1\n",
      "1074 1\n",
      "1075 1\n",
      "1076 1\n",
      "1077 1\n",
      "1078 1\n",
      "1079 1\n",
      "1080 1\n",
      "1081 1\n",
      "1082 1\n",
      "1083 1\n",
      "1084 1\n",
      "1085 1\n",
      "1086 1\n",
      "1087 1\n",
      "1088 1\n",
      "1089 1\n",
      "1090 1\n",
      "1091 1\n",
      "1092 1\n",
      "1093 1\n",
      "1094 1\n",
      "1095 1\n",
      "1096 1\n",
      "1097 1\n",
      "1098 1\n",
      "1099 1\n",
      "1100 1\n",
      "1101 1\n",
      "1102 1\n",
      "1103 1\n",
      "1104 1\n",
      "1105 1\n",
      "1106 1\n",
      "1107 1\n",
      "1108 1\n",
      "1109 1\n",
      "1110 1\n",
      "1111 1\n",
      "1112 1\n",
      "1113 1\n",
      "1114 1\n",
      "1115 1\n",
      "1116 1\n",
      "1117 1\n",
      "1118 1\n",
      "1119 1\n",
      "1120 1\n",
      "1121 1\n",
      "1122 1\n",
      "1123 1\n",
      "1124 1\n",
      "1125 1\n",
      "1126 1\n",
      "1127 1\n",
      "1128 1\n",
      "1129 1\n",
      "1130 1\n",
      "1131 1\n",
      "1132 1\n",
      "1133 1\n",
      "1134 1\n",
      "1135 1\n",
      "1136 1\n",
      "1137 1\n",
      "1138 1\n",
      "1139 1\n",
      "1140 1\n",
      "1141 1\n",
      "1142 1\n",
      "1143 1\n",
      "1144 1\n",
      "1145 1\n",
      "1146 1\n",
      "1147 1\n",
      "1148 1\n",
      "1149 1\n",
      "1150 1\n",
      "1151 1\n",
      "1152 1\n",
      "1153 1\n",
      "1154 1\n",
      "1155 1\n",
      "1156 1\n",
      "1157 1\n",
      "1158 1\n",
      "1159 1\n",
      "1160 1\n",
      "1161 1\n",
      "1162 1\n",
      "1163 1\n",
      "1164 1\n",
      "1165 1\n",
      "1166 1\n",
      "1167 1\n",
      "1168 1\n",
      "1169 1\n",
      "1170 1\n",
      "1171 1\n",
      "1172 1\n",
      "1173 1\n",
      "1174 1\n",
      "1175 1\n",
      "1176 1\n",
      "1177 1\n",
      "1178 1\n",
      "1179 1\n",
      "1180 1\n",
      "1181 1\n",
      "1182 1\n",
      "1183 1\n",
      "1184 1\n",
      "1185 1\n",
      "1186 1\n",
      "1187 1\n",
      "1188 1\n",
      "1189 1\n",
      "1190 1\n",
      "1191 1\n",
      "1192 1\n",
      "1193 1\n",
      "1194 1\n",
      "1195 1\n",
      "1196 1\n",
      "1197 1\n",
      "1198 1\n",
      "1199 1\n",
      "1200 1\n",
      "1201 1\n",
      "1202 1\n",
      "1203 1\n",
      "1204 1\n",
      "1205 1\n",
      "1206 1\n",
      "1207 1\n",
      "1208 1\n",
      "1209 1\n",
      "1210 1\n",
      "1211 1\n",
      "1212 1\n",
      "1213 1\n",
      "1214 1\n",
      "1215 1\n",
      "1216 1\n",
      "1217 1\n",
      "1218 1\n",
      "1219 1\n",
      "1220 1\n",
      "1221 1\n",
      "1222 1\n",
      "1223 1\n",
      "1224 1\n",
      "1225 1\n",
      "1226 1\n",
      "1227 1\n",
      "1228 1\n",
      "1229 1\n",
      "1230 1\n",
      "1231 1\n",
      "1232 1\n",
      "1233 1\n",
      "1234 1\n",
      "1235 1\n",
      "1236 1\n",
      "1237 1\n",
      "1238 1\n",
      "1239 1\n",
      "1240 1\n",
      "1241 1\n",
      "1242 1\n",
      "1243 1\n",
      "1244 1\n",
      "1245 1\n",
      "1246 1\n",
      "1247 1\n",
      "1248 1\n",
      "1249 1\n",
      "1250 1\n",
      "1251 1\n",
      "1252 1\n",
      "1253 1\n",
      "1254 1\n",
      "1255 1\n",
      "1256 1\n",
      "1257 1\n",
      "1258 1\n",
      "1259 1\n",
      "1260 1\n",
      "1261 1\n",
      "1262 1\n",
      "1263 1\n",
      "1264 1\n",
      "1265 1\n",
      "1266 1\n",
      "1267 1\n",
      "1268 1\n",
      "1269 1\n",
      "1270 1\n",
      "1271 1\n",
      "1272 1\n",
      "1273 1\n",
      "1274 1\n",
      "1275 1\n",
      "1276 1\n",
      "1277 1\n",
      "1278 1\n",
      "1279 1\n",
      "1280 1\n",
      "1281 1\n",
      "1282 1\n",
      "1283 1\n",
      "1284 1\n",
      "1285 1\n",
      "1286 1\n",
      "1287 1\n",
      "1288 1\n",
      "1289 1\n",
      "1290 1\n",
      "1291 1\n",
      "1292 1\n",
      "1293 1\n",
      "1294 1\n",
      "1295 1\n",
      "1296 1\n",
      "1297 1\n",
      "1298 1\n",
      "1299 1\n",
      "1300 1\n",
      "1301 1\n",
      "1302 1\n",
      "1303 1\n",
      "1304 1\n",
      "1305 1\n",
      "1306 1\n",
      "1307 1\n",
      "1308 1\n",
      "1309 1\n"
     ]
    }
   ],
   "execution_count": 430
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-25T09:15:48.599095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "# 타이타닉 데이터셋 함수 가져오기\n",
    "train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "def get_data():\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=wandb.config.batch_size)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=wandb.config.batch_size, shuffle=False)\n",
    "    return train_data_loader, validation_data_loader, test_data_loader\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, activation_fn):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            activation_fn,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            activation_fn,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_model_and_optimizer(activation_fn):\n",
    "    model = MyModel(n_input=11, n_output=2, activation_fn=activation_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    n_epochs = wandb.config.epochs\n",
    "    best_validation_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for train_batch in train_data_loader:\n",
    "            input, target = train_batch['input'], train_batch['target']\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_data_loader)\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input, target = validation_batch['input'], validation_batch['target']\n",
    "                output = model(input)\n",
    "                loss = loss_fn(output, target)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = validation_loss / len(validation_data_loader)\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Logging train and validation loss to wandb\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Loss\": avg_train_loss,\n",
    "            \"Validation Loss\": avg_validation_loss\n",
    "        })\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}\")\n",
    "\n",
    "    return best_model_state, best_validation_loss\n",
    "\n",
    "def test_model(model, test_data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": range(892, 892 + len(predictions)),\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"submission.csv has been created.\")\n",
    "\n",
    "def main(args):\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [20, 20],\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_training\",\n",
    "        name=f\"Run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "    activation_functions = {\n",
    "        \"ELU\": nn.ELU(),\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"LeakyReLU\": nn.LeakyReLU(0.01),\n",
    "        \"PReLU\": nn.PReLU()\n",
    "    }\n",
    "\n",
    "    best_validation_loss = float(\"inf\")\n",
    "    best_activation_fn = None\n",
    "\n",
    "    for name, activation_fn in activation_functions.items():\n",
    "        print(f\"Training with {name} activation function.\")\n",
    "        model, optimizer = get_model_and_optimizer(activation_fn)\n",
    "        model_state, validation_loss = training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "        print(f\"{name} Validation Loss: {validation_loss:.4f}\")\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_activation_fn = activation_fn\n",
    "            best_model_state = model_state\n",
    "\n",
    "    print(f\"Best Activation Function: {best_activation_fn} with Validation Loss: {best_validation_loss:.4f}\")\n",
    "\n",
    "    # 최적의 활성화 함수로 모델 초기화 및 로드 후 테스트 예측\n",
    "    final_model = MyModel(n_input=11, n_output=2, activation_fn=best_activation_fn)\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "    test_model(final_model, test_data_loader)\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=True, help=\"Use wandb logging\")\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=100, help=\"Batch size (int, default: 100)\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1000, help=\"Number of training epochs (int, default: 1000)\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args)\n"
   ],
   "id": "3ee6664a2ecb435c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alstj\\AppData\\Local\\Temp\\ipykernel_16408\\928423758.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\alstj\\AppData\\Local\\Temp\\ipykernel_16408\\377400784.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\alstj\\Desktop\\대학교\\3학년\\2학기\\딥러닝\\git\\deep\\link_dl_HomeWork\\_02_homeworks\\HW_2\\wandb\\run-20241025_181548-rvnzwt2o</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training/runs/rvnzwt2o' target=\"_blank\">Run_2024-10-25_18-15-48</a></strong> to <a href='https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training' target=\"_blank\">https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training/runs/rvnzwt2o' target=\"_blank\">https://wandb.ai/alstjr7141-korea-university-of-technology-and-education/titanic_training/runs/rvnzwt2o</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with ELU activation function.\n",
      "Epoch 100, Training Loss: 0.4142, Validation Loss: 0.4279\n",
      "Epoch 200, Training Loss: 0.3989, Validation Loss: 0.4163\n",
      "Epoch 300, Training Loss: 0.3689, Validation Loss: 0.4091\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![결과](https://github.com/MinSeokCSE/Deep/blob/main/kaggle.png?raw=true) ",
   "id": "d25bbc6363c9628a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "411c9e660c9eeb8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
